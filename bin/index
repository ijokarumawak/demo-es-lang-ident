#!venv/bin/python

"""
A script to index documents for the language identification search demo. This
will index the same documents in multiple ways in order to demonstrate the
various indexing and search strategies. By default, this will use the
WiLI-2018 training set corpus.

Due to duplicate strings in the dataset, it will also calculate a hash of
the text to use as document ID, ensuring no duplicate text is in the index.
In order to ensure that we index the number of requested documents, we will
maintain an in-memory list of index document IDs, which are hashes of the
contents.
"""

import argparse
import copy
import hashlib
import json
import os
import certifi

from elasticsearch import Elasticsearch, helpers
from timeit import default_timer as timer
from ssl import create_default_context


ANALYZERS = {
    'de': 'german_custom',
    'en': 'english',
    'ja': 'kuromoji',
    'ko': 'nori',
    'zh': 'smartcn',
}
DEFAULT_CORPUS = os.path.join('data', 'WiLI-2018', 'x_train.txt')
DEFAULT_MAX = None
DEFAULT_URL = 'http://localhost:9200'
STRATEGIES = ['lang-per-field', 'lang-per-index']


class Timer:
    def __enter__(self):
        self.start = timer()
        return self

    def __exit__(self, *args):
        self.end = timer()
        self.interval = self.end - self.start


def file_length(filename):
    """
    Count the number of lines in a file.
    See: https://gist.github.com/zed/0ac760859e614cd03652#file-gistfile1-py-L48-L49
    """
    return sum(1 for _ in open(filename, 'r'))


def load_mapping(name):
    with open(os.path.join('config', 'mappings', f'{name}.json'), 'r') as f:
        return json.load(f)


def load_german_analyzer():
    with open(os.path.join('config', 'mappings', 'de-analyzer.json'), 'r') as f:
        return json.load(f)


def recreate_per_field_index(es):
    """Recreates the index settings and mappings for the language per-field strategy."""

    index = 'lang-per-field'
    mapping = load_mapping(index)

    # inject the german analyzer stanza
    mapping['settings']['analysis'] = load_german_analyzer()['settings']['analysis']

    print(f"Recreating index for strategy: {index}")
    es.indices.delete(index=index, ignore=404)
    es.indices.create(index=index, body=mapping)


def recreate_per_index_indices(es):
    """Recreates the index settings and mappings for the language per-index strategy."""

    def create(lang, index, analyzer):
        body = copy.deepcopy(mapping)

        if lang == 'de':
            # inject the german analyzer stanza
            body['settings']['analysis'] = load_german_analyzer()['settings']['analysis']

        # set the analyzer on the text field based on language of the index
        body['mappings']['properties']['contents']['properties']['text']['analyzer'] = analyzer

        print(f" - {index}")
        es.indices.delete(index=index, ignore=404)
        es.indices.create(index=index, body=body)

    strategy = 'lang-per-index'
    mapping = load_mapping(strategy)
    print(f"Recreating indices for strategy: {strategy}")

    # create the default index for unsupported languages
    create(None, strategy, 'default')

    for lang, analyzer in ANALYZERS.items():
        create(lang, f'{strategy}_{lang}', analyzer)


def recreate_pipeline(es, id):
    """Recreates a pipeline of the given pipeline ID."""
    def body():
        with open(os.path.join('config', 'pipelines', f'{id}.json'), 'r') as f:
            return json.load(f)

    print(f"Recreating pipeline: {id}")
    es.ingest.delete_pipeline(id=id, ignore=404)
    es.ingest.put_pipeline(id=id, body=body())


def index_docs(es, index, corpus, max_num_docs=None):
    """
    Indexes all documents in the corpus to the given index (up to `max`). Indexing will use the pipeline of the same
    name as the index.
    """

    def actions(limit):
        counter = 0
        ids = set()
        with open(corpus, 'r') as f:
            for line in f:
                if max_num_docs and counter >= limit:
                    break

                # ensure no duplicate documents
                id = hashlib.md5(line.encode()).hexdigest()
                if id not in ids:
                    counter += 1
                    ids.add(id)
                    yield {
                        '_index': index,
                        '_id': id,
                        'pipeline': index,
                        '_source': {'contents': line},
                    }

    num_docs = file_length(corpus)

    print(f"Indexing documents into '{index}'")
    print(f" - corpus size: {num_docs}")

    if max_num_docs:
        max_num_docs = min(max_num_docs, num_docs)
        print(f" - number of docs to index: {max_num_docs}")

    with Timer() as t:
        # index in bulk with large chunks since the documents are very small, and a big timeout to just get it done
        helpers.bulk(es, actions(max_num_docs), chunk_size=10000, request_timeout=600, refresh='wait_for')
    print(f" - duration: {t.interval:.04f} sec")


def print_statistics(es):
    """Prints statistics about all indices."""

    def p(indices, index):
        count = indices[index]['total']['docs']['count']
        print(f" - {index}: {count}")

    stats = es.indices.stats(index='lang-*')
    print(f"Index statistics:")

    total = stats['_all']['total']['docs']['count']
    print(f" - total docs: {total}")

    indices = stats['indices']
    p(indices, 'lang-per-field')
    p(indices, 'lang-per-index')
    for lang in ANALYZERS.keys():
        p(indices, f'lang-per-index_{lang}')


def main():
    parser = argparse.ArgumentParser(prog='index')
    parser.add_argument('--url', default=DEFAULT_URL, help="An Elasticsearch connection URL, e.g. http://user:secret@localhost:9200")
    parser.add_argument('--corpus', default=DEFAULT_CORPUS, help="Corpus to index, in a line delimited plain text file")
    parser.add_argument('--max', type=int, default=DEFAULT_MAX, help="The maximum number of corpus documents to index, e.g. 1000")
    args = parser.parse_args()

    context = create_default_context(cafile=certifi.where())
    es = Elasticsearch(args.url, ssl_context=context)

    recreate_per_field_index(es)
    recreate_per_index_indices(es)

    for s in STRATEGIES:
        recreate_pipeline(es, s)

    for s in STRATEGIES:
        index_docs(es, s, args.corpus, args.max)

    print_statistics(es)


if __name__ == "__main__":
    main()
